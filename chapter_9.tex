\section{Conditional Generation}
\marginnote{\footnotesize{Original blog post see \url{https://www.spaces.ac.cn/archives/9257}}}

In the previous few sections, we have mainly been focusing on discussing the theoretical frameworks for the diffusion models. In this section, we shall focus on a more practical topic, that is the conditional generation of new samples with the diffusion models. 

As a generative model, diffusion model shares a similar developmental track as VAE, GAN and the flow model, for which the conditional generative model was soon proposed after the birth of the unconditional generative model. Generally speaking, the unconditional generation can facilitate the exploration of the upper limit in the qualities of the samples that are generated by the model, whereas the conditional generation is more for practical applications, as now we can control the samples that are being generated based on our specific needs. Since the birth of DDPM, many different conditional diffusion generative models have been developed. The capability to conditionally control the diffusion generative process is the driving force that makes the diffusion models become so popular these days, such as the text-to-image models DALL.E2 and Imagen.

In this section, we will briefly study and summarise the theoretical foundations for the conditional diffusion models.

\subsection{Theoretical analysis}

Practically, there are two pathways to realise conditional generation, which are usually dubbed as either \textbf{Classifier-Guidance} or \textbf{Classifier-Free}. 

For most people, the cost for training a diffusion model at the SOTA-level is too large. On the other hand, the cost associated with training a classifier is more or less acceptable. As such, most people wants to use a pre-trained diffusion model and add in an additional classifier to realise the conditional sample generation, This is the \emph{ad-hoc} \textbf{Classifier-Guidance} approach. Nevertheless, for Tech Giants such as Google and OpenAI, that are not lack of both data and computational power, they are more keen to include the conditional signals during the training of the diffusion model, in order to achieve better sample qualities, this is the so-called \textbf{Classifier-Free} approach.

The \textbf{Classifier-Guidance} approach was first proposed in the article ``\emph{Diffusion Models Beats GANs on Image Synthesis}''\cite{dhariwal2021diffusion}, which was aimed to achieve sample generations according to different classifications.  Subsequently, in the paper ``More Control for Free! Image Synthesis with Semantic Diffusion Guidance''\cite{liu2023more}, the concept of the classifier has been further generalised, making it possible to generate samples that are guided by either images or texts. Generally speaking, the training costs for the Classifier-Guidance models are lower. However, the cost for inferencing is higher and the sample quality is lower compared the Classifier-Free approach.

As for the \textbf{Classifier-Free} approach, it was first proposed in the article ``\emph{Classifier-Free Diffusion Guidance}''\cite{ho2022classifier}. Models such as DALL.E2 and Imagen are all based on this approach. Strictly speaking, there is not much technical tricks involved in the Classifier-Free approach. It is the most straightforward generalisation of diffusion model to its conditional variant. It appears later than the Classifier-Guidance approach most likely because of its high training cost. Nevertheless, when there exists sufficient data and computational power for training, the samples generated from the Classifier-Free approach have demonstrated amazing capability in controlling the details in the process of sample generation. 

\subsection{Conditional input}
Simply put, there is not much technicalities in the Classifier-Free approach, except for its high training costs. As such, we will focus the subsequent discussions on the Classifier-Guidance approach, and briefly outline the Classifier-Free approach at the end. 

Based on the analysis from the previous sections, it should be clear to the readers that the key step in the generation process is to build the transition probability $p(\xtm|\xt)$. As such, for sample generation based on some condition $\bm{y}$, all we need to do is introduce $\bm{y}$  into the generation process via the conditional probability $p(\xtm|\xt,\bm{y})$ which replaces $p(\xtm|\xt)$. In order to reuse the $p(\xtm|\xt)$. from the trained unconditional generative model, we apply the Baye's rule
\begin{equation}
    \label{eq:9.1}
    p(\xtm|\bm{y})=\frac{p(\xtm)p(\bm{y}|\xtm)}{p(\bm{y})},
\end{equation}
and putting $\xt$ back into every term, we get:
\begin{equation}
    \label{eq:9.2}
    p(\xtm|\xt,\bm{y})=\frac{p(\xtm|\xt)p(\bm{y}|\xtm,\xt)}{p(\bm{y}|\xt)}.
\end{equation}
Note that $\xt$ is generated from $\xtm$ by adding the noise in the forward process. In this case, adding $\xt$ will not bring any benefit to the classifier, as such, $p(\bm{y}|\xtm,\xt)=p(\bm{y}|\xtm)$, this leads to the following:
\begin{equation}
    \label{eq:9.3}
    p(\xtm|\xt,\bm{y})=\frac{p(\xtm|\xt)p(\bm{y}|\xtm)}{p(\bm{y}|\xt)}=p(\xtm|\xt)\exp\left[\log p(\bm{y}|\xtm)-\log p(\bm{y}|\xt) \right].
\end{equation}

\subsection{Approximated distribution}
Readers who are familiar with the derivations in \cref{sect_5:SDE} on SDE will not be surprised by the following derivations. When $T$ is sufficiently large, the standard deviations for the distribution $p(\xt|\xtm)$ will be sufficiently small. This is saying that the transition probability $p(\xt|\xtm)$ will only be significantly larger than zero when $\xt$ is very close to $\xtm$. This is also true for the conditional probabilities, $i.e.$ either $p(\xtm|\xt,\bm{y})$ or $p(\xt|\xtm,\bm{y})$ will only be significantly larger than zero when $\xt$ is very close to $\xtm$. As such, we  only need to consider the change in the probability distributions within this small range of $\x$. With this, we can apply the Taylor expansion as 
\begin{equation}
    \label{eq:9.4}
    \log p(\bm{y}|\xtm)-\log p(\bm{y}|\xt) \approx (\xtm-\xt)\nabla_{\xt}\log p(\bm{y}|\xt).
\end{equation}
Strictly speaking, there should also exist an extra $t$-dependent term, but it is irrelevant to $\xtm$, as it is a constant that will not change the probability of $\xtm$. Therefore, this term is ignored in \cref{eq:9.4}. Assuming that we already have $p(\xtm|\xt)=\mathcal{N}(\xtm;\bm{\mu}(\xt),\sigmat^2\bm{I})\propto\exp\left[-\|\xtm-\bm{\mu}(\xt)\|^2/2\sigmat^2\right]$, then we have the following approximation:
\begin{align}
    p(\xtm|\xt,\bm{y})&\propto \exp\left[-\|\xtm-\bm{\mu}(\xt)\|^2/2\sigmat^2 +(\xtm-\xt)\nabla_{\xt}\log p(\bm{y}|\xt)\right]\nonumber\\
    &\propto \exp\left[-\|\xtm-\bm{\mu}(\xt) - \sigmat^2\nabla_{\xt}\log p(\bm{y}|\xt)\|^2/2\sigmat^2\log p(\bm{y}|\xt)\right].\label{eq:9.5}
\end{align}
This result shows that the distribution $p(\xtm|\xt,\bm{y})$ is approximately a normal distribution of $\mathcal{N}(\xtm;\bm{\mu}(\xt)+\sigmat^2\nabla_{\xt}\log p(\bm{y}|\xt),
\sigmat^2\bm{I})$. In this case, we only need to modify the sampling process to:
\begin{equation}
    \label{eq:9.6}
    \xtm=\bm{\mu}(\xt)+\sigmat^2\nabla_{\xt}\log p(\bm{y}|\xt)+\sigmat\bm{\varepsilon},\qquad \bm{\varepsilon}\sim\normdist.
\end{equation}
This is the core result in the Classifier-Guidance Approach. Note that $\xt$ is the sample after noises are being added, which is the input for $p(\bm{y}|\xt)$. It means that we need a model that can predict the noised sample. If we only have a model to predict the noise-free sample $p_0(\bm{y}|\x_0)$, then what we way consider is 
\begin{equation}
    \label{eq:9.7}
    p(\bm{y}|\x_0)=p_{0}(y|\bm{\mu}(\xt)),
\end{equation}
$i.e.$, we use $\bm{\mu}(\cdot)$ to denoise $\xt$ before plugging it into $p(\bm{y}|\xt)$. This avoids the cost of training a classifier with noisy samples.

\subsection{Scaling the classifier gradient}
In the original paper entitled ``\emph{Diffusion Model Beats GAN on Image Synthesis}'', it was found that if a scaling parameter $\gamma$ was introduced into the gradient of the classifier, one can better attenuate the generated samples. In this case, we have:
\begin{equation}
    \label{eq:9.8}
    \xtm=\bm{\mu}(\xt)+\sigmat^2\gamma\nabla_{\xt}\log p(\bm{y}|\xt)+\sigmat\bm{\varepsilon},\qquad \bm{\varepsilon}\sim\normdist.
\end{equation}
When $\gamma>1$, the generation process will utilise more information from the classifier. This will increase the correlation between the output and the input condition $\bm{y}$, at the price of reducing the diversity of the output samples. On the other hand, when $\gamma<1$, the generative process will reduce the correlation between the generated output and the conditional input, thus leading to  more diversified outcomes. 

How should we understand this parameter? In the original paper, it was proposed that $\gamma$ could be understood by adjusting the form of the probability distribution through the use of a power operation:
\begin{equation}
    \label{eq:9.9}
    \tilde{p}(\bm{y}|\xt)=\frac{p^{\gamma}(\bm{y}|\xt)}{\mathcal{Z}(\xt)},\qquad \mathcal{Z}(\xt)=\sum_{\bm{y}}p^{\gamma}(\bm{y}|\xt).
\end{equation}
As $\gamma$ increases, $ \tilde{p}(\bm{y}|\xt)$ will slowly merge into an one-hot distribution. If this distribution was used to substitute $p(\bm{y}|\x_0)$ in the classifier as the guidance, then the generative process will tend to select samples that have higher confidence level in being correctly classified. Although this explanation can facilitate our understanding to some extent, it was  not completely right. This is because
\begin{equation}
    \label{eq:9.10}
    \nabla_{\xt}\log \tilde{p}(\bm{y}|\xt)=\gamma\nabla_{\xt}\log p(\bm{y}|\xt)-\nabla_{\xt}\mathcal{Z}(\xt)\neq \gamma\nabla_{\xt}\log p(\bm{y}|\xt).
\end{equation}
The authors of the original paper had wrongly took $\mathcal{Z}(\xt)$ as a constant, which gave $\nabla_{\xt}\mathcal{Z}(\xt)=0$. However, when $\gamma\neq 1$, it is obvious that $\mathcal{Z}(\xt)$ will also be dependent on $\xt$. Unfortunately, there does not seem to be a suitable solution to  overcome this problem, we can only roughly assume that the property of the gradient function at $\gamma=1$ can be generalised to the case when $\gamma\neq 1$.

\subsection{Similarity control}

As a matter of fact, the best way to understand the case when $\gamma\neq 1$ is to abandon the Bayes' rule [\cref{eq:9.2} and \cref{eq:9.3}] when trying to understand the term $p(\xtm|\xt,\bm{y})$. Instead, we could straightforwardly define
\begin{equation}
    \label{eq:9.11}
    p(\xtm|\xt,\bm{y})=\frac{p(\xtm|\xt)e^{\gamma\cdot\mbox{\scriptsize{sim}}(\xtm,\bm{y})}}{\mathcal{Z}(\xt,\bm{y})},\quad \mathcal{Z}(\xt|\bm{y})=\sum_{\xtm}p(\xtm|\xt)e^{\gamma\cdot\mbox{\scriptsize{sim}}(\xtm,\bm{y})},
\end{equation}
in which $\mbox{sim}(\xtm,\bm{y})$ is a similarity measure between the generated sample $\xtm$ and the conditional input $\bm{y}$. From this perspective, $\gamma$ comes naturally from the definition of  $p(\xtm|\xt,\bm{y})$, which directly controls the correlation between the output and the condition. As $\gamma$ increases, the model will generate $\xtm$ that has stronger correlation with $\bm{y}$.

In order to arrive at an approximation that will further facilitate samplings, we could expand the similarity term approximately the point $\xtm$ that is close to $\xt$ as:
\begin{equation}
    \label{eq:9.12}
    e^{\gamma\cdot\mbox{\scriptsize{sim}}(\xtm,\bm{y})}\approx e^{\gamma\cdot\mbox{\scriptsize{sim}}(\xt,\bm{y}) + \gamma(\xtm-\xt)\nabla_{\xt} \mbox{\scriptsize{sim}}(\xt,\bm{y})}.
\end{equation}
Assuming that this approximation is sufficiently accurate, we can then eliminate the term that are irrelevant to $\xtm$, from which we get
\begin{equation}
    \label{eq:9.13}
    p(\xtm|\xt,\bm{y})\propto p(\xtm|\xt)e^{ \gamma(\xtm-\xt)\nabla_{\xt} \mbox{\scriptsize{sim}}(\xt,\bm{y})}.
\end{equation}
Similarly, substituting in $p(\xtm|\xt)=\mathcal{N}(\xtm;\bm{\mu}(\xt),\sigmat^2\bm{I})$ and completing the square, we get
\begin{equation}
    \label{eq:9.14}
    p(\xtm|\xt)\propto\mathcal{N}(\xtm;\bm{\mu}(\xt)+\gamma\sigmat^2\nabla_{\xt}\mbox{sim}(\xt,\bm{y}),\sigmat^2\bm{I}),
\end{equation}
which now we no longer need to worry about the probabilistic interpretation of $p(\bm{y}|\xt)$, but only a suitable definition of the similarity measure $\mbox{sim}(\xt,\bm{y})$. Here, $\bm{y}$ becomes no longer restricted to a class, it could be any type of inputs (image or text). The general treatment is to encode them into a vector representation using their own encoder and then apply the cosine similarity:
\begin{equation}
    \label{eq:9.15}
    \mbox{sim}(\xt,\bm{y})=\frac{E_{1}(\xt)\cdot E_{2}(\bm{y})}{\|E_{1}(\xt)\|\cdot\|E_{2}(\bm{y})\|}.
\end{equation}
It should be noted that, since the intermediate state $\xt$ contains Gaussian noise, it is generally better not to use the encoder for the noise-free sample but fine-tune it with noised samples. Above is the core idea in the article ``\emph{More Control for Free! Image Synthesis with Semantic Diffusion Guidance}''.

\subsection{Continuous scenarios}
We saw that in all the previous derivations, that the correction to the mean is either $\sigmat^2\gamma\nabla_{\xt}\log p(\bm{y}|\xt)$ or $\gamma\sigmat^2\nabla_{\xt}\mbox{sim}(\xt,\bm{y})$, which both contain the term $\sigmat$, meaning that when $\sigmat=0$, the correcting term will also be zero, making the conditional input playing no effect on sample generation.

So in this case, can we still set $\sigmat$ to be zero? The answer is yes. For example, in DDIM introduced in \cref{sect_4:DDIM}, it has its corresponding generation process with $\sigmat=0$. How should we realise conditional generation? In this case, we need to apply the SDE formalism introduced in \cref{sect:6}, in which we showed that for a forward SDE:
\begin{equation}
    \label{eq:9.16}
    d\x=f_t (\x)dt+g_t d\bm{w},
\end{equation}
with the corresponding reverse SDE
\begin{equation}
    \label{eq:9.17}
    d\x=\left(f_t(\x)-\frac{1}{2}(g_t^2+\sigmat^2)\nabla_{\xt}\log p_t(\x)\right)dt+\sigmat d\bm{w}.
\end{equation}
Here, we are allowed to choose $\sigmat$ freely. Both DDPM and DDIM can be considered as its special cases. Especially when $\sigmat=0$, this corresponds to a generalised DDIM. It can be seen that, in the reverse SDE, the term that is input-dependent is $\nabla_{\xt}\log p_t(\x)$. If now we want to perform conditional generation, we should just need to change it to $\nabla_{\x}\log p_t(\x|\bm{y})$. From Bayes' rule, we have
\begin{equation}
    \label{eq:9.18}
    \nabla_{\x}\log p_t(\x|\bm{y}) = \nabla_{x}\log p_t (\x)+\nabla_{\bm{x}}\log p_t (\bm{y}|\x),
\end{equation}
noting that $\nabla_{\x}\log p_t (\bm{y})=0$. With the parameterisation trick that we had used before, we have $\nabla_{x}\log p_t (\x)=-\bm{\varepsilon}_{\bm{\theta}}(\xt,t)/\betatbar$, thus we have
\begin{equation}
    \label{eq:9.19}
    \nabla_{\x}\log p_t(\x|\bm{y}) = -\frac{\bm{\varepsilon}_{\bm{\theta}}(\xt,t)}{\betatbar}+\nabla_{\bm{x}}\log p_t (\bm{y}|\x)=-\frac{\bm{\varepsilon}_{\bm{\theta}}(\xt,t)-\betatbar\nabla_{\bm{x}}\log p_t (\bm{y}|\x) }{\betatbar}.
\end{equation}
This means that, regardless of what the variance $\sigmat$ is, all we need to do is to replace $\bm{\varepsilon}_{\bm{\theta}}(\xt,t)$ with $\bm{\varepsilon}_{\bm{\theta}}(\xt,t)-\betatbar\nabla_{\bm{x}}\log p_t (\bm{y}|\x)$, and we can realise conditional generation. As such, under the unified framework of SDE, we could arrive at the generation framework of Classifier-Guidance very easily.

\subsection{Classifier-free approach}
Finally, we will briefly introduce the Classifier-Free approach. It is actually very simple, we just need to define
\begin{equation}
    \label{eq:9.20}
    p(\xtm|\xt,\bm{y})=\mathcal{N}(\xtm;\bm{\mu}(\xt,\bm{y}),\sigmat^2\bm{I}).
\end{equation}
Using the DDPM results from the previous sections, we can parameterise $\bm{\mu}(\xt,\bm{y})$ generally as 
\begin{equation}
    \label{eq:9.21}
    \bm{\mu}(\xt,\bm{y})=\frac{1}{\alphat}\left(\xt - \frac{\betat^2}{\betatbar^2}\bm{\varepsilon}_{\bm{\theta}}(\xt,\bm{y},t) \right),
\end{equation}
with the corresponding loss function 
\begin{equation}
    \label{eq:9.22}
\mathbb{E}_{\x_0,\bm{y}\sim\tilde{p}(\x_0\bm{y}),\bm{\varepsilon}\sim\normdist}\left[\|\bm{\varepsilon} \|_2^2 -\bm{\varepsilon}_{\bm{\theta}}(\xt,\bm{y},t) \right].
\end{equation}
The advantage here is that the additional input $\bm{y}$ is already included in the training. Theoretically, the more information we have, the easier it is to train. On the other hand, this also brings a major disadvantage, whereby the model needs to be retrained for every new set of conditional input signals. 

More specifically, like the Classifier-Guidances, the Classifier-Free approach also introduced a $\gamma$-parameter to balance specificity and diversity. Practically, we rewrite \cref{eq:9.8} using the following modified mean:
\begin{equation}
    \label{eq:9.23}
    \bm{\mu}(\xt)+\sigma_t^2\gamma\nabla_{\xt}\log p(\bm{y}|\xt)=\gamma\left[\bm{\mu}(\xt)+\sigma_t^2\nabla_{\xt}\log p(\bm{y}|\xt) \right] -(\gamma-1)\bm{\mu}(\xt),
\end{equation}
which shows that the Classifier-Free approach is equivalent to the usage of a trainable model to directly regress the term $\bm{\mu}(\xt)+\sigma_t^2\nabla_{\xt}\log p(\bm{y}|\xt)$. In comparison, we can also introduce $\omega=\gamma-1$ in the Classifier-Guidance and use the following
\begin{equation}
    \hat{\bm{\varepsilon}}_{\bm{\theta}}=(1+\omega)\bm{\varepsilon}_{\bm{\theta}}(\xt,\bm{y},t)-\omega \bm{\varepsilon}_{\bm{\theta}}(\xt,t)
\end{equation}
to replace $\bm{\varepsilon}_{\bm{\theta}}(\xt,\bm{y},t)$ in the generation. How should we obtain the unconditioned $\bm{\varepsilon}_{\bm{\theta}}(\xt,t)$ in this case? We can introduce a specific input $\bm{\phi}$, which the target image is all the image samples. Once this is added for model training, we can treat that $\bm{\varepsilon}_{\bm{\theta}}(\xt,t)=\bm{\varepsilon}_{\bm{\theta}}(\xt,\bm{\phi},t)$.